{
  "request": {
    "language": "en",
    "chat_api_url": "http://123.129.219.111:3000/v1/",
    "api_key": "sk-h6nyfmxUx70YUMBQNwaayrmXda62L7rxvytMNshxACWVzJXe",
    "model": "gpt-4o",
    "target": "给我随意符合逻辑的5个算子，过滤，去重！",
    "json_file": "/mnt/DataFlow/lz/proj/DataFlow/dataflow/example/GeneralTextPipeline/translation.jsonl",
    "python_file_path": "/mnt/DataFlow/lz/proj/DataFlow-Agent/dataflow_agent/tmps/dXNlcm5hbWU9bGl1emhvdQ==/my_pipeline.py",
    "need_debug": false,
    "max_debug_rounds": 2,
    "use_local_model": false,
    "local_model_path": "",
    "cache_dir": "/mnt/DataFlow/lz/proj/DataFlow-Agent/cache_dir",
    "session_id": "dXNlcm5hbWU9bGl1emhvdQ=="
  },
  "messages": [
    {
      "content": "\nYou are a data processing expert. Please generate a structured JSON report according to the user's question.\nBased on the user's knowledge base data, you will recommend a suitable data processing pipeline composed of multiple processing nodes.\nYou need to analyze the user's data types and content, then recommend an appropriate pipeline accordingly.\n",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "system",
      "name": null,
      "id": "e7aaccb8-5b0e-4b38-9968-e4193bc1423d"
    },
    {
      "content": "\n[ROLE] You are a data governance workflow recommendation system.\nYou need to automatically select appropriate operator nodes and assemble a complete data processing pipeline based on contextual information.\n\n[INPUT]\nYou will receive the following information:\nThe requirements that the pipeline must meet:\n========\n给我随意符合逻辑的5个算子，过滤，去重！\n========\nSample data information:\n========\n[{'raw_content': 'The study investigates the impact of data quality on the performance of large language models.'}]\n========\nThe list of available operators for each data type:\n============================\n\"PromptedGenerator\":\"基于用户提供的提示词（prompt）生成数据。结合系统提示词和输入内容生成符合要求的输出文本。输入参数：\n- llm_serving：LLM服务对象，需实现LLMServingABC接口\n- system_prompt：系统提示词，定义模型行为，默认为'You are a helpful agent.'\n- input_key：输入内容字段名，默认为'raw_content'\n- output_key：输出生成内容字段名，默认为'generated_content'\n输出参数：\n- 包含生成内容的DataFrame\n- 返回输出字段名，用于后续算子引用\"; \"PairedPromptedGenerator\":\"PairedPromptedGenerator：基于两列配对输入（input_key_1 与 input_key_2）进行成对提示生成。\n算子会将 system_prompt 与每行的两列文本按固定模板拼接后，调用 LLM 服务批量生成结果，并将模型输出写回到 DataFrame 的指定列。\n\n输入参数：\n- llm_serving：LLM 服务对象（实现 LLMServingABC 接口）\n- system_prompt：系统提示词（默认 'You are a helpful agent.'）。该提示会放在每条样本前缀，  用于约束模型的角色与输出风格。\n- input_key_1：第一列输入字段名（默认 'input_key_1'）\n- input_key_2：第二列输入字段名（默认 'input_key_2'）\n- output_key：输出字段名（默认 'generated_content'）\n\n处理逻辑：\n1) 从 storage 中读取名为 'dataframe' 的 DataFrame；\n2) 对于每一行，若 input_key_1 与 input_key_2 均非空，则按模板：\n   system_prompt + input_key_1 + 值 + '\\n' + input_key_2 + 值\n   构造 LLM 输入；\n3) 批量调用 llm_serving.generate_from_input 生成文本；\n4) 将生成结果写入 DataFrame 的 output_key 列并保存。\n\n输出：\n- 返回写入了生成结果的新 DataFrame（由 storage 管理保存），\n- 返回 output_key 以便后续算子引用。\"; \"RandomDomainKnowledgeRowGenerator\":\"\"; \"Text2QAGenerator\":\"该算子用于为给定的文档片段生成种子QA对。\n\n输入参数：\n- input_key: 包含文档片段的字段名\n- prompt_key: 包含提示词的字段名\n- output_quesion_key: 包含生成问题的字段名\n- output_answer_key: 包含生成答案的字段名\n\"; \"Text2MultiHopQAGenerator\":\"('MultiHopQAGenerator 是多跳问答对生成处理器，支持从文本中自动生成需要多步推理的问题与答案。', '处理流程包括：文本预处理、信息抽取、问题生成与回答生成，支持自定义语言模型后端和参数。', '输出格式如下：', '输入：\\ntext: <原始上下文文本>', '输出：\\n{\\n  \"text\": <处理后的文本字符串>,\\n  \"qa_pairs\": [\\n    {\\n      \"question\": <字符串：生成的问题>,\\n      \"reasoning_steps\": [\\n        {\"step\": <推理过程的步骤 1>},\\n        {\"step\": <步骤 2>} ...\\n      ],\\n      \"answer\": <字符串：最终答案>,\\n      \"supporting_facts\": [<支持该答案的事实 1>, <事实 2>, ...],\\n      \"type\": <可选：问题类型，如“生物学”、“历史”等>\\n    },\\n    ...\\n  ],\\n  \"metadata\": {\\n    \"source\": <数据来源>,\\n    \"timestamp\": <时间戳字符串>,\\n    \"complexity\": <整数：问题复杂度标记>\\n  }\\n}')\"; \"EmbeddingGenerator\":\"EmbeddingGenerator算子用于从输入文本生成向量表示（embedding），通常用于语义检索、聚类或下游模型输入等任务。\n\n输入参数：\n- embedding_serving：Embedding服务对象，需实现LLMServingABC接口，用于生成文本的向量表示\n- input_key：输入文本字段名，默认为'text'\n- output_key：输出向量字段名，默认为'embeddings'\n\n输出参数：\n- 包含文本向量的DataFrame，每行对应一个输入文本的embedding\n- 返回输出字段名（如'embeddings'），可供后续算子引用\"; \"BenchDatasetEvaluator\":\"该算子用于对比预测答案与标准答案的匹配度，支持两种评估模式：\n\n1. 字符串匹配（match）：使用数学验证方法比较答案，适用于有明确答案的问题\n2. 语义匹配（semantic）：使用LLM评估答案的语义相似度，适用于开放性问题\n\n输入参数：\n- input_test_answer_key：预测答案字段名\n- input_gt_answer_key：标准答案字段名\n- input_question_key：问题字段名（语义匹配模式下必需）\n- compare_method：比较方法（match/semantic）\n\n输出参数：\n- answer_match_result：匹配结果（True/False）\n- 统计结果将保存到指定的eval_result_path路径\n\"; \"Text2QASampleEvaluator\":\"该算子用于为给的的文档片段生成种子QA对打分\n\n输入参数：\n- input_question_key: Field name containing the generated question\n- input_answer_key: Field name containing the generated answer\n- output_question_quality_key: Field name containing the question quality grade\n- output_question_quality_feedback_key: Field name containing the question quality feedback\n- output_answer_alignment_key: Field name containing the answer alignment grade\n- output_answer_alignment_feedback_key: Field name containing the answer alignment feedback\n- output_answer_verifiability_key: Field name containing the answer verifiability grade\n- output_downstream_value_key: Field name containing the downstream value grade\n- output_downstream_value_feedback_key: Field name containing the downstream value feedback\n\"; \"PromptedEvaluator\":\"PromptedEvaluator：使用 LLM 根据系统提示词对数据质量进行评分，并将评分写回 DataFrame（同时通过 storage 持久化）。模型应只输出分数（整数）。\n功能：对每行输入文本生成一个评分。\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口。\n- system_prompt：系统提示词（默认：'Please evaluate the quality of this data on a scale from 1 to 5.'）。\n- input_key：输入文本所在列名（默认：'raw_content'）。\n- output_key：评分结果写入的列名（默认：'eval'）。\n输出：\n- 返回输出列名（用于后续算子引用），评分结果已写回并保存。\"; \"PromptedFilter\":\"PromptedFilter 使用内置的 PromptedEvaluator 对输入数据进行数值化打分，并根据指定的分数区间（min_score 到 max_score，闭区间）筛选出符合条件的样本。默认情况下打分范围是 1–5，但用户可以通过 system_prompt 自定义其他评分规则。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，定义评估规范（可选，默认 'Please evaluate the quality of this data on a scale from 1 to 5.'）\n- input_key：待评估文本所在列名（默认 'raw_content'）\n- output_key：写回打分结果的列名（默认 'eval'，若已存在将被覆盖）\n- min_score：筛选的最小分（默认 5）\n- max_score：筛选的最大分（默认 5）\n\n输出参数：\n- 过滤后的 DataFrame（仅保留分数位于 [min_score, max_score] 的行）\n- 返回 output_key 以供后续算子引用\n\n备注：\n- 默认打分区间是 1–5，但可根据实际 prompt 改变。\"; \"KCenterGreedyFilter\":\"该算子用于从大量的文档片段中选取部分文档片段，用于后续生成种子QA对\n\n输入参数:\n- input_key: 包含文档片段的字段名\n- embedding_model_path: 嵌入模型路径\n- num_samples: 选取的文档片段数量\n- method: 选择方法，随机或k-center-greedy\n\n\"; \"GeneralFilter\":\"该算子支持通过多个自定义函数对 DataFrame 进行灵活过滤。\n\n每条过滤规则是一个函数（例如 lambda 表达式），接受一个 DataFrame 并返回一个布尔类型的 Series，用于指定保留哪些行。\n\n输入参数：\n- filter_rules：一个函数列表，每个函数形式为 lambda df: ...，需返回一个与 df 长度一致的布尔 Series。所有规则之间采用与（AND）关系组合。\n\n示例：\n  - lambda df: df['score'] > 0.5\n  - lambda df: df['label'].isin(['A', 'B'])\"; \"PromptedRefiner\":\"PromptedRefiner 根据给定的 system_prompt 对指定列的文本进行改写/润色/规范化，并将结果**就地写回**同一列（覆盖原内容）。其做法是对每一行拼接 `system_prompt + raw_content` 作为模型输入，批量生成改写结果。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，用于描述改写目标与风格（默认 'You are a helpful agent.'）\n- input_key：要改写的文本列名（默认 'raw_content'），改写后会覆盖该列\n\n输出参数：\n- 覆盖后的 DataFrame（同名列被改写后的文本）\n- 无返回值（结果已通过 DataFlowStorage 写出）\n\n备注：\n- 该算子**覆盖** input_key 列；若需保留原文，建议先拷贝到新列。\n- 期望每行在 input_key 列提供可用文本；空值将不会生成对应输入，如与行数不匹配可能导致赋值报错。\"; \"PandasOperator\":\"该算子支持通过多个自定义函数对 DataFrame 进行任意操作（如添加列、重命名、排序等）。\n\n每个函数（通常为 lambda 表达式）接受一个 DataFrame 并返回一个修改后的 DataFrame。\n\n输入参数：\n- process_fn：一个函数列表，每个函数形式为 lambda df: ...，必须返回一个 DataFrame。\n\n示例：\n  - lambda df: df.assign(score2=df['score'] * 2)\n  - lambda df: df.sort_values('score', ascending=False)\"\n============================\n\n[key rules]\n1. Follow Execution Order:\n  Data generation must occur before data extraction\n  Data extraction must occur before data validation\n  Correct order: Filter → Generate → Extract → Validate\n  Incorrect order: Filter → Extract → Generate ❌\n2 .Validate Data Availability:\n\n  Check sample data to confirm which fields already exist\n  If an operator requires field \"X\" but it's not present in the sample data, ensure a preceding operator creates it\n\n[Common Error Patterns to Avoid]\n❌ Incorrect Example: [\"FilterData\", \"ExtractAnswer\", \"GenerateAnswer\"]\nProblem: Attempting to extract the answer before it is generated\n\n✅ Correct Example: [\"FilterData\", \"GenerateAnswer\", \"ExtractAnswer\"]\nReason: Generate first, then extract\n\n❌ Incorrect Example: [\"ValidateAnswer\", \"GenerateAnswer\"]\nProblem: Validating an answer that does not exist yet\n\n✅ Correct Example: [\"GenerateAnswer\", \"ExtractAnswer\", \"ValidateAnswer\"]\nReason: Complete data flow\n\n\n[OUTPUT RULES]\n1. Please select suitable operator nodes for each type and return them in the following JSON format:\n{\n  \"ops\": [\"OperatorA\", \"OperatorB\", \"OperatorC\"],\n  \"reason\": \"State your reasoning here. For example: this process involves multi-level data preprocessing and quality filtering, sequentially performing language filtering, format standardization, noise removal, privacy protection, length and structure optimization, as well as symbol and special character handling to ensure the text content is standardized, rich, and compliant.\"\n}\n2 Only the names of the operators are needed.\n3. Please verify whether the selected operators and their order fully meet the requirements specified in 给我随意符合逻辑的5个算子，过滤，去重！.\n[Question]\nBased on the above rules, what pipeline should be recommended???\n",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "e1ce5668-fd5b-4615-ab7f-ff89eed5b552",
      "example": false
    },
    {
      "content": "```json\n{\n  \"ops\": [\"PromptedFilter\", \"GeneralFilter\", \"PromptedRefiner\", \"EmbeddingGenerator\", \"PandasOperator\"],\n  \"reason\": \"The pipeline involves filtering, refining, and generating embeddings for the data. The PromptedFilter is used to filter data based on quality scores. The GeneralFilter is applied for additional custom filtering rules. The PromptedRefiner refines the text content to ensure it is standardized. The EmbeddingGenerator creates vector representations of the text for further analysis. Finally, the PandasOperator is used for any additional data manipulation tasks, such as deduplication, ensuring the data is clean and ready for downstream tasks.\"\n}\n```",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 2964,
          "total_tokens": 3106,
          "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 0,
            "rejected_prediction_tokens": 0
          },
          "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
          }
        },
        "model_name": "gpt-4o-2024-08-06",
        "system_fingerprint": "fp_4a331a0222",
        "id": "chatcmpl-CV5VbenYmmYgsZXTAVFhchksLkyLq",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--829f3731-3d04-4100-b6e3-a02e3e95f53d-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 2964,
        "output_tokens": 142,
        "total_tokens": 3106,
        "input_token_details": {
          "audio": 0,
          "cache_read": 0
        },
        "output_token_details": {
          "audio": 0,
          "reasoning": 0
        }
      }
    }
  ],
  "agent_results": {
    "classifier": {
      "pre_tool_results": {
        "sample": [
          {
            "raw_content": "We evaluate the system on multiple benchmarks, including WMT and TED talks."
          },
          {
            "raw_content": "The model was trained using a combination of supervised and unsupervised learning techniques."
          }
        ],
        "categories": [
          "agentic_rag",
          "chemistry",
          "code",
          "conversations",
          "core_speech",
          "core_text",
          "core_vision",
          "db",
          "general_text",
          "knowledge_cleaning",
          "rare",
          "reasoning",
          "text2sql",
          "text_pt",
          "text_sft"
        ]
      },
      "post_tools": [],
      "results": {
        "category": "core_text"
      }
    },
    "recommender": {
      "pre_tool_results": {
        "sample": [
          {
            "raw_content": "The study investigates the impact of data quality on the performance of large language models."
          }
        ],
        "target": "给我随意符合逻辑的5个算子，过滤，去重！",
        "operator": "\"PromptedGenerator\":\"基于用户提供的提示词（prompt）生成数据。结合系统提示词和输入内容生成符合要求的输出文本。输入参数：\n- llm_serving：LLM服务对象，需实现LLMServingABC接口\n- system_prompt：系统提示词，定义模型行为，默认为'You are a helpful agent.'\n- input_key：输入内容字段名，默认为'raw_content'\n- output_key：输出生成内容字段名，默认为'generated_content'\n输出参数：\n- 包含生成内容的DataFrame\n- 返回输出字段名，用于后续算子引用\"; \"PairedPromptedGenerator\":\"PairedPromptedGenerator：基于两列配对输入（input_key_1 与 input_key_2）进行成对提示生成。\n算子会将 system_prompt 与每行的两列文本按固定模板拼接后，调用 LLM 服务批量生成结果，并将模型输出写回到 DataFrame 的指定列。\n\n输入参数：\n- llm_serving：LLM 服务对象（实现 LLMServingABC 接口）\n- system_prompt：系统提示词（默认 'You are a helpful agent.'）。该提示会放在每条样本前缀，  用于约束模型的角色与输出风格。\n- input_key_1：第一列输入字段名（默认 'input_key_1'）\n- input_key_2：第二列输入字段名（默认 'input_key_2'）\n- output_key：输出字段名（默认 'generated_content'）\n\n处理逻辑：\n1) 从 storage 中读取名为 'dataframe' 的 DataFrame；\n2) 对于每一行，若 input_key_1 与 input_key_2 均非空，则按模板：\n   system_prompt + input_key_1 + 值 + '\\n' + input_key_2 + 值\n   构造 LLM 输入；\n3) 批量调用 llm_serving.generate_from_input 生成文本；\n4) 将生成结果写入 DataFrame 的 output_key 列并保存。\n\n输出：\n- 返回写入了生成结果的新 DataFrame（由 storage 管理保存），\n- 返回 output_key 以便后续算子引用。\"; \"RandomDomainKnowledgeRowGenerator\":\"\"; \"Text2QAGenerator\":\"该算子用于为给定的文档片段生成种子QA对。\n\n输入参数：\n- input_key: 包含文档片段的字段名\n- prompt_key: 包含提示词的字段名\n- output_quesion_key: 包含生成问题的字段名\n- output_answer_key: 包含生成答案的字段名\n\"; \"Text2MultiHopQAGenerator\":\"('MultiHopQAGenerator 是多跳问答对生成处理器，支持从文本中自动生成需要多步推理的问题与答案。', '处理流程包括：文本预处理、信息抽取、问题生成与回答生成，支持自定义语言模型后端和参数。', '输出格式如下：', '输入：\\ntext: <原始上下文文本>', '输出：\\n{\\n  \"text\": <处理后的文本字符串>,\\n  \"qa_pairs\": [\\n    {\\n      \"question\": <字符串：生成的问题>,\\n      \"reasoning_steps\": [\\n        {\"step\": <推理过程的步骤 1>},\\n        {\"step\": <步骤 2>} ...\\n      ],\\n      \"answer\": <字符串：最终答案>,\\n      \"supporting_facts\": [<支持该答案的事实 1>, <事实 2>, ...],\\n      \"type\": <可选：问题类型，如“生物学”、“历史”等>\\n    },\\n    ...\\n  ],\\n  \"metadata\": {\\n    \"source\": <数据来源>,\\n    \"timestamp\": <时间戳字符串>,\\n    \"complexity\": <整数：问题复杂度标记>\\n  }\\n}')\"; \"EmbeddingGenerator\":\"EmbeddingGenerator算子用于从输入文本生成向量表示（embedding），通常用于语义检索、聚类或下游模型输入等任务。\n\n输入参数：\n- embedding_serving：Embedding服务对象，需实现LLMServingABC接口，用于生成文本的向量表示\n- input_key：输入文本字段名，默认为'text'\n- output_key：输出向量字段名，默认为'embeddings'\n\n输出参数：\n- 包含文本向量的DataFrame，每行对应一个输入文本的embedding\n- 返回输出字段名（如'embeddings'），可供后续算子引用\"; \"BenchDatasetEvaluator\":\"该算子用于对比预测答案与标准答案的匹配度，支持两种评估模式：\n\n1. 字符串匹配（match）：使用数学验证方法比较答案，适用于有明确答案的问题\n2. 语义匹配（semantic）：使用LLM评估答案的语义相似度，适用于开放性问题\n\n输入参数：\n- input_test_answer_key：预测答案字段名\n- input_gt_answer_key：标准答案字段名\n- input_question_key：问题字段名（语义匹配模式下必需）\n- compare_method：比较方法（match/semantic）\n\n输出参数：\n- answer_match_result：匹配结果（True/False）\n- 统计结果将保存到指定的eval_result_path路径\n\"; \"Text2QASampleEvaluator\":\"该算子用于为给的的文档片段生成种子QA对打分\n\n输入参数：\n- input_question_key: Field name containing the generated question\n- input_answer_key: Field name containing the generated answer\n- output_question_quality_key: Field name containing the question quality grade\n- output_question_quality_feedback_key: Field name containing the question quality feedback\n- output_answer_alignment_key: Field name containing the answer alignment grade\n- output_answer_alignment_feedback_key: Field name containing the answer alignment feedback\n- output_answer_verifiability_key: Field name containing the answer verifiability grade\n- output_downstream_value_key: Field name containing the downstream value grade\n- output_downstream_value_feedback_key: Field name containing the downstream value feedback\n\"; \"PromptedEvaluator\":\"PromptedEvaluator：使用 LLM 根据系统提示词对数据质量进行评分，并将评分写回 DataFrame（同时通过 storage 持久化）。模型应只输出分数（整数）。\n功能：对每行输入文本生成一个评分。\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口。\n- system_prompt：系统提示词（默认：'Please evaluate the quality of this data on a scale from 1 to 5.'）。\n- input_key：输入文本所在列名（默认：'raw_content'）。\n- output_key：评分结果写入的列名（默认：'eval'）。\n输出：\n- 返回输出列名（用于后续算子引用），评分结果已写回并保存。\"; \"PromptedFilter\":\"PromptedFilter 使用内置的 PromptedEvaluator 对输入数据进行数值化打分，并根据指定的分数区间（min_score 到 max_score，闭区间）筛选出符合条件的样本。默认情况下打分范围是 1–5，但用户可以通过 system_prompt 自定义其他评分规则。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，定义评估规范（可选，默认 'Please evaluate the quality of this data on a scale from 1 to 5.'）\n- input_key：待评估文本所在列名（默认 'raw_content'）\n- output_key：写回打分结果的列名（默认 'eval'，若已存在将被覆盖）\n- min_score：筛选的最小分（默认 5）\n- max_score：筛选的最大分（默认 5）\n\n输出参数：\n- 过滤后的 DataFrame（仅保留分数位于 [min_score, max_score] 的行）\n- 返回 output_key 以供后续算子引用\n\n备注：\n- 默认打分区间是 1–5，但可根据实际 prompt 改变。\"; \"KCenterGreedyFilter\":\"该算子用于从大量的文档片段中选取部分文档片段，用于后续生成种子QA对\n\n输入参数:\n- input_key: 包含文档片段的字段名\n- embedding_model_path: 嵌入模型路径\n- num_samples: 选取的文档片段数量\n- method: 选择方法，随机或k-center-greedy\n\n\"; \"GeneralFilter\":\"该算子支持通过多个自定义函数对 DataFrame 进行灵活过滤。\n\n每条过滤规则是一个函数（例如 lambda 表达式），接受一个 DataFrame 并返回一个布尔类型的 Series，用于指定保留哪些行。\n\n输入参数：\n- filter_rules：一个函数列表，每个函数形式为 lambda df: ...，需返回一个与 df 长度一致的布尔 Series。所有规则之间采用与（AND）关系组合。\n\n示例：\n  - lambda df: df['score'] > 0.5\n  - lambda df: df['label'].isin(['A', 'B'])\"; \"PromptedRefiner\":\"PromptedRefiner 根据给定的 system_prompt 对指定列的文本进行改写/润色/规范化，并将结果**就地写回**同一列（覆盖原内容）。其做法是对每一行拼接 `system_prompt + raw_content` 作为模型输入，批量生成改写结果。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，用于描述改写目标与风格（默认 'You are a helpful agent.'）\n- input_key：要改写的文本列名（默认 'raw_content'），改写后会覆盖该列\n\n输出参数：\n- 覆盖后的 DataFrame（同名列被改写后的文本）\n- 无返回值（结果已通过 DataFlowStorage 写出）\n\n备注：\n- 该算子**覆盖** input_key 列；若需保留原文，建议先拷贝到新列。\n- 期望每行在 input_key 列提供可用文本；空值将不会生成对应输入，如与行数不匹配可能导致赋值报错。\"; \"PandasOperator\":\"该算子支持通过多个自定义函数对 DataFrame 进行任意操作（如添加列、重命名、排序等）。\n\n每个函数（通常为 lambda 表达式）接受一个 DataFrame 并返回一个修改后的 DataFrame。\n\n输入参数：\n- process_fn：一个函数列表，每个函数形式为 lambda df: ...，必须返回一个 DataFrame。\n\n示例：\n  - lambda df: df.assign(score2=df['score'] * 2)\n  - lambda df: df.sort_values('score', ascending=False)\""
      },
      "post_tools": [
        "combine_pipeline"
      ],
      "results": {
        "ops": [
          "PromptedFilter",
          "GeneralFilter",
          "PromptedRefiner",
          "EmbeddingGenerator",
          "PandasOperator"
        ],
        "reason": "The pipeline involves filtering, refining, and generating embeddings for the data. The PromptedFilter is used to filter data based on quality scores. The GeneralFilter is applied for additional custom filtering rules. The PromptedRefiner refines the text content to ensure it is standardized. The EmbeddingGenerator creates vector representations of the text for further analysis. Finally, the PandasOperator is used for any additional data manipulation tasks, such as deduplication, ensuring the data is clean and ready for downstream tasks."
      }
    },
    "pipeline_builder": {
      "pre_tool_results": {
        "recommendation": []
      },
      "post_tools": [],
      "results": {
        "success": null,
        "skipped": true,
        "file_path": "/mnt/DataFlow/lz/proj/DataFlow-Agent/dataflow_agent/tmps/dXNlcm5hbWU9bGl1emhvdQ==/my_pipeline.py"
      }
    },
    "exporter": {
      "pre_tool_results": {
        "nodes_info": [
          {
            "id": "node1",
            "name": "PromptedFilter",
            "type": "filter",
            "config": {
              "init": {
                "llm_serving": "self.llm_serving",
                "system_prompt": "Please evaluate the quality of this data on a scale from 1 to 5.",
                "min_score": 1,
                "max_score": 5
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "raw_content",
                "output_key": "eval"
              }
            }
          },
          {
            "id": "node2",
            "name": "GeneralFilter",
            "type": "filter",
            "config": {
              "init": {
                "filter_rules": null
              },
              "run": {
                "storage": "self.storage.step()"
              }
            }
          },
          {
            "id": "node3",
            "name": "PromptedRefiner",
            "type": "refine",
            "config": {
              "init": {
                "llm_serving": "self.llm_serving",
                "system_prompt": "You are a helpful agent."
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "raw_content"
              }
            }
          },
          {
            "id": "node4",
            "name": "EmbeddingGenerator",
            "type": "generate",
            "config": {
              "init": {
                "embedding_serving": null
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "text",
                "output_key": "embeddings"
              }
            }
          },
          {
            "id": "node5",
            "name": "PandasOperator",
            "type": "refine",
            "config": {
              "init": {
                "process_fn": null
              },
              "run": {
                "storage": "self.storage.step()"
              }
            }
          }
        ]
      },
      "post_tools": [],
      "results": {
        "nodes": [
          {
            "id": "node1",
            "name": "PromptedFilter",
            "type": "filter",
            "config": {
              "init": {
                "llm_serving": "self.llm_serving",
                "system_prompt": "Please evaluate the quality of this data on a scale from 1 to 5.",
                "min_score": 1,
                "max_score": 5
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "raw_content",
                "output_key": "step1"
              }
            }
          },
          {
            "id": "node2",
            "name": "GeneralFilter",
            "type": "filter",
            "config": {
              "init": {
                "filter_rules": null
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "step1",
                "output_key": "step2"
              }
            }
          },
          {
            "id": "node3",
            "name": "PromptedRefiner",
            "type": "refine",
            "config": {
              "init": {
                "llm_serving": "self.llm_serving",
                "system_prompt": "You are a helpful agent."
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "step2",
                "output_key": "step3"
              }
            }
          },
          {
            "id": "node4",
            "name": "EmbeddingGenerator",
            "type": "generate",
            "config": {
              "init": {
                "embedding_serving": null
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "step3",
                "output_key": "step4"
              }
            }
          },
          {
            "id": "node5",
            "name": "PandasOperator",
            "type": "refine",
            "config": {
              "init": {
                "process_fn": null
              },
              "run": {
                "storage": "self.storage.step()",
                "input_key": "step4",
                "output_key": "output_final"
              }
            }
          }
        ]
      }
    }
  },
  "temp_data": {
    "round": 0,
    "pre_tool_results": {
      "sample": [
        {
          "raw_content": "The study investigates the impact of data quality on the performance of large language models."
        }
      ],
      "target": "给我随意符合逻辑的5个算子，过滤，去重！",
      "operator": "\"PromptedGenerator\":\"基于用户提供的提示词（prompt）生成数据。结合系统提示词和输入内容生成符合要求的输出文本。输入参数：\n- llm_serving：LLM服务对象，需实现LLMServingABC接口\n- system_prompt：系统提示词，定义模型行为，默认为'You are a helpful agent.'\n- input_key：输入内容字段名，默认为'raw_content'\n- output_key：输出生成内容字段名，默认为'generated_content'\n输出参数：\n- 包含生成内容的DataFrame\n- 返回输出字段名，用于后续算子引用\"; \"PairedPromptedGenerator\":\"PairedPromptedGenerator：基于两列配对输入（input_key_1 与 input_key_2）进行成对提示生成。\n算子会将 system_prompt 与每行的两列文本按固定模板拼接后，调用 LLM 服务批量生成结果，并将模型输出写回到 DataFrame 的指定列。\n\n输入参数：\n- llm_serving：LLM 服务对象（实现 LLMServingABC 接口）\n- system_prompt：系统提示词（默认 'You are a helpful agent.'）。该提示会放在每条样本前缀，  用于约束模型的角色与输出风格。\n- input_key_1：第一列输入字段名（默认 'input_key_1'）\n- input_key_2：第二列输入字段名（默认 'input_key_2'）\n- output_key：输出字段名（默认 'generated_content'）\n\n处理逻辑：\n1) 从 storage 中读取名为 'dataframe' 的 DataFrame；\n2) 对于每一行，若 input_key_1 与 input_key_2 均非空，则按模板：\n   system_prompt + input_key_1 + 值 + '\\n' + input_key_2 + 值\n   构造 LLM 输入；\n3) 批量调用 llm_serving.generate_from_input 生成文本；\n4) 将生成结果写入 DataFrame 的 output_key 列并保存。\n\n输出：\n- 返回写入了生成结果的新 DataFrame（由 storage 管理保存），\n- 返回 output_key 以便后续算子引用。\"; \"RandomDomainKnowledgeRowGenerator\":\"\"; \"Text2QAGenerator\":\"该算子用于为给定的文档片段生成种子QA对。\n\n输入参数：\n- input_key: 包含文档片段的字段名\n- prompt_key: 包含提示词的字段名\n- output_quesion_key: 包含生成问题的字段名\n- output_answer_key: 包含生成答案的字段名\n\"; \"Text2MultiHopQAGenerator\":\"('MultiHopQAGenerator 是多跳问答对生成处理器，支持从文本中自动生成需要多步推理的问题与答案。', '处理流程包括：文本预处理、信息抽取、问题生成与回答生成，支持自定义语言模型后端和参数。', '输出格式如下：', '输入：\\ntext: <原始上下文文本>', '输出：\\n{\\n  \"text\": <处理后的文本字符串>,\\n  \"qa_pairs\": [\\n    {\\n      \"question\": <字符串：生成的问题>,\\n      \"reasoning_steps\": [\\n        {\"step\": <推理过程的步骤 1>},\\n        {\"step\": <步骤 2>} ...\\n      ],\\n      \"answer\": <字符串：最终答案>,\\n      \"supporting_facts\": [<支持该答案的事实 1>, <事实 2>, ...],\\n      \"type\": <可选：问题类型，如“生物学”、“历史”等>\\n    },\\n    ...\\n  ],\\n  \"metadata\": {\\n    \"source\": <数据来源>,\\n    \"timestamp\": <时间戳字符串>,\\n    \"complexity\": <整数：问题复杂度标记>\\n  }\\n}')\"; \"EmbeddingGenerator\":\"EmbeddingGenerator算子用于从输入文本生成向量表示（embedding），通常用于语义检索、聚类或下游模型输入等任务。\n\n输入参数：\n- embedding_serving：Embedding服务对象，需实现LLMServingABC接口，用于生成文本的向量表示\n- input_key：输入文本字段名，默认为'text'\n- output_key：输出向量字段名，默认为'embeddings'\n\n输出参数：\n- 包含文本向量的DataFrame，每行对应一个输入文本的embedding\n- 返回输出字段名（如'embeddings'），可供后续算子引用\"; \"BenchDatasetEvaluator\":\"该算子用于对比预测答案与标准答案的匹配度，支持两种评估模式：\n\n1. 字符串匹配（match）：使用数学验证方法比较答案，适用于有明确答案的问题\n2. 语义匹配（semantic）：使用LLM评估答案的语义相似度，适用于开放性问题\n\n输入参数：\n- input_test_answer_key：预测答案字段名\n- input_gt_answer_key：标准答案字段名\n- input_question_key：问题字段名（语义匹配模式下必需）\n- compare_method：比较方法（match/semantic）\n\n输出参数：\n- answer_match_result：匹配结果（True/False）\n- 统计结果将保存到指定的eval_result_path路径\n\"; \"Text2QASampleEvaluator\":\"该算子用于为给的的文档片段生成种子QA对打分\n\n输入参数：\n- input_question_key: Field name containing the generated question\n- input_answer_key: Field name containing the generated answer\n- output_question_quality_key: Field name containing the question quality grade\n- output_question_quality_feedback_key: Field name containing the question quality feedback\n- output_answer_alignment_key: Field name containing the answer alignment grade\n- output_answer_alignment_feedback_key: Field name containing the answer alignment feedback\n- output_answer_verifiability_key: Field name containing the answer verifiability grade\n- output_downstream_value_key: Field name containing the downstream value grade\n- output_downstream_value_feedback_key: Field name containing the downstream value feedback\n\"; \"PromptedEvaluator\":\"PromptedEvaluator：使用 LLM 根据系统提示词对数据质量进行评分，并将评分写回 DataFrame（同时通过 storage 持久化）。模型应只输出分数（整数）。\n功能：对每行输入文本生成一个评分。\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口。\n- system_prompt：系统提示词（默认：'Please evaluate the quality of this data on a scale from 1 to 5.'）。\n- input_key：输入文本所在列名（默认：'raw_content'）。\n- output_key：评分结果写入的列名（默认：'eval'）。\n输出：\n- 返回输出列名（用于后续算子引用），评分结果已写回并保存。\"; \"PromptedFilter\":\"PromptedFilter 使用内置的 PromptedEvaluator 对输入数据进行数值化打分，并根据指定的分数区间（min_score 到 max_score，闭区间）筛选出符合条件的样本。默认情况下打分范围是 1–5，但用户可以通过 system_prompt 自定义其他评分规则。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，定义评估规范（可选，默认 'Please evaluate the quality of this data on a scale from 1 to 5.'）\n- input_key：待评估文本所在列名（默认 'raw_content'）\n- output_key：写回打分结果的列名（默认 'eval'，若已存在将被覆盖）\n- min_score：筛选的最小分（默认 5）\n- max_score：筛选的最大分（默认 5）\n\n输出参数：\n- 过滤后的 DataFrame（仅保留分数位于 [min_score, max_score] 的行）\n- 返回 output_key 以供后续算子引用\n\n备注：\n- 默认打分区间是 1–5，但可根据实际 prompt 改变。\"; \"KCenterGreedyFilter\":\"该算子用于从大量的文档片段中选取部分文档片段，用于后续生成种子QA对\n\n输入参数:\n- input_key: 包含文档片段的字段名\n- embedding_model_path: 嵌入模型路径\n- num_samples: 选取的文档片段数量\n- method: 选择方法，随机或k-center-greedy\n\n\"; \"GeneralFilter\":\"该算子支持通过多个自定义函数对 DataFrame 进行灵活过滤。\n\n每条过滤规则是一个函数（例如 lambda 表达式），接受一个 DataFrame 并返回一个布尔类型的 Series，用于指定保留哪些行。\n\n输入参数：\n- filter_rules：一个函数列表，每个函数形式为 lambda df: ...，需返回一个与 df 长度一致的布尔 Series。所有规则之间采用与（AND）关系组合。\n\n示例：\n  - lambda df: df['score'] > 0.5\n  - lambda df: df['label'].isin(['A', 'B'])\"; \"PromptedRefiner\":\"PromptedRefiner 根据给定的 system_prompt 对指定列的文本进行改写/润色/规范化，并将结果**就地写回**同一列（覆盖原内容）。其做法是对每一行拼接 `system_prompt + raw_content` 作为模型输入，批量生成改写结果。\n\n输入参数：\n- llm_serving：LLM 服务对象，需实现 LLMServingABC 接口\n- system_prompt：系统提示词，用于描述改写目标与风格（默认 'You are a helpful agent.'）\n- input_key：要改写的文本列名（默认 'raw_content'），改写后会覆盖该列\n\n输出参数：\n- 覆盖后的 DataFrame（同名列被改写后的文本）\n- 无返回值（结果已通过 DataFlowStorage 写出）\n\n备注：\n- 该算子**覆盖** input_key 列；若需保留原文，建议先拷贝到新列。\n- 期望每行在 input_key 列提供可用文本；空值将不会生成对应输入，如与行数不匹配可能导致赋值报错。\"; \"PandasOperator\":\"该算子支持通过多个自定义函数对 DataFrame 进行任意操作（如添加列、重命名、排序等）。\n\n每个函数（通常为 lambda 表达式）接受一个 DataFrame 并返回一个修改后的 DataFrame。\n\n输入参数：\n- process_fn：一个函数列表，每个函数形式为 lambda df: ...，必须返回一个 DataFrame。\n\n示例：\n  - lambda df: df.assign(score2=df['score'] * 2)\n  - lambda df: df.sort_values('score', ascending=False)\""
    },
    "recommender_instance": {
      "tool_manager": {
        "role_pre_tools": {
          "classifier": {
            "sample": {},
            "categories": {}
          },
          "recommender": {
            "sample": {},
            "target": {},
            "operator": {}
          },
          "exporter": {
            "nodes_info": {}
          }
        },
        "role_post_tools": {
          "recommender": [
            {
              "name": "combine_pipeline",
              "description": "Combine pipeline post tool for recommender",
              "args_schema": "{'__module__': 'dataflow_agent.workflow.wf_pipeline_recommend_extract_json', '__annotations__': {'oplist': 'list'}, 'model_config': {}, '__class_vars__': set(), '__private_attributes__': {}, '__weakref__': <attribute '__weakref__' of 'GetOpInput' objects>, '__doc__': None, '__abstractmethods__': frozenset(), '_abc_impl': <_abc._abc_data object at 0x7f72f3e84a00>, '__pydantic_custom_init__': False, '__pydantic_post_init__': None, '__pydantic_setattr_handlers__': {}, '__pydantic_decorators__': DecoratorInfos(validators={}, field_validators={}, root_validators={}, field_serializers={}, model_serializers={}, model_validators={}, computed_fields={}), '__pydantic_generic_metadata__': {'origin': None, 'args': (), 'parameters': ()}, '__pydantic_complete__': True, '__pydantic_parent_namespace__': {'builder': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f02050>, 'cls_get_sample': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f02080>, 'cls_get_categories': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f020b0>, 'rec_get_sample': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f02110>, 'rec_get_target': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f02140>, 'rec_get_operator': <pydantic._internal._model_construction._PydanticWeakRef object at 0x7f72f0f022c0>}, '__pydantic_fields__': {'oplist': FieldInfo(annotation=list, required=True, description=\"list['xxx']的算子列表\")}, '__pydantic_computed_fields__': {}, '__pydantic_core_schema__': {'type': 'model', 'cls': <class 'dataflow_agent.workflow.wf_pipeline_recommend_extract_json.create_pipeline_graph.<locals>.GetOpInput'>, 'schema': {'type': 'model-fields', 'fields': {'oplist': {'type': 'model-field', 'schema': {'type': 'list', 'items_schema': {'type': 'any'}}, 'metadata': {'pydantic_js_updates': {'description': \"list['xxx']的算子列表\"}}}}, 'model_name': 'GetOpInput', 'computed_fields': []}, 'custom_init': False, 'root_model': False, 'config': {'title': 'GetOpInput'}, 'ref': 'dataflow_agent.workflow.wf_pipeline_recommend_extract_json.create_pipeline_graph.<locals>.GetOpInput:94299104189360', 'metadata': {'pydantic_js_functions': [<bound method BaseModel.__get_pydantic_json_schema__ of <class 'dataflow_agent.workflow.wf_pipeline_recommend_extract_json.create_pipeline_graph.<locals>.GetOpInput'>>]}}, '__pydantic_validator__': SchemaValidator(title=\"GetOpInput\", validator=Model(\n    ModelValidator {\n        revalidate: Never,\n        validator: ModelFields(\n            ModelFieldsValidator {\n                fields: [\n                    Field {\n                        name: \"oplist\",\n                        lookup_key_collection: LookupKeyCollection {\n                            by_name: Simple(\n                                LookupPath {\n                                    first_item: PathItemString {\n                                        key: \"oplist\",\n                                        py_key: Py(\n                                            0x00007f72f0f12b30,\n                                        ),\n                                    },\n                                    rest: [],\n                                },\n                            ),\n                            by_alias: None,\n                            by_alias_then_name: None,\n                        },\n                        name_py: Py(\n                            0x00007f72f3e84e70,\n                        ),\n                        validator: List(\n                            ListValidator {\n                                strict: false,\n                                item_validator: None,\n                                min_length: None,\n                                max_length: None,\n                                name: OnceLock(\n                                    <uninit>,\n                                ),\n                                fail_fast: false,\n                            },\n                        ),\n                        frozen: false,\n                    },\n                ],\n                model_name: \"GetOpInput\",\n                extra_behavior: Ignore,\n                extras_validator: None,\n                extras_keys_validator: None,\n                strict: false,\n                from_attributes: false,\n                loc_by_alias: true,\n                validate_by_alias: None,\n                validate_by_name: None,\n            },\n        ),\n        class: Py(\n            0x000055c3b89b9bb0,\n        ),\n        generic_origin: None,\n        post_init: None,\n        frozen: false,\n        custom_init: false,\n        root_model: false,\n        undefined: Py(\n            0x00007f7304d93460,\n        ),\n        name: \"GetOpInput\",\n    },\n), definitions=[], cache_strings=True), '__pydantic_serializer__': SchemaSerializer(serializer=Model(\n    ModelSerializer {\n        class: Py(\n            0x000055c3b89b9bb0,\n        ),\n        serializer: Fields(\n            GeneralFieldsSerializer {\n                fields: {\n                    \"oplist\": SerField {\n                        key_py: Py(\n                            0x00007f72f3e84e70,\n                        ),\n                        alias: None,\n                        alias_py: None,\n                        serializer: Some(\n                            List(\n                                ListSerializer {\n                                    item_serializer: Any(\n                                        AnySerializer,\n                                    ),\n                                    filter: SchemaFilter {\n                                        include: None,\n                                        exclude: None,\n                                    },\n                                    name: \"list[any]\",\n                                },\n                            ),\n                        ),\n                        required: true,\n                        serialize_by_alias: None,\n                        serialization_exclude_if: None,\n                    },\n                },\n                computed_fields: Some(\n                    ComputedFields(\n                        [],\n                    ),\n                ),\n                mode: SimpleDict,\n                extra_serializer: None,\n                filter: SchemaFilter {\n                    include: None,\n                    exclude: None,\n                },\n                required_fields: 1,\n            },\n        ),\n        has_extra: false,\n        root_model: false,\n        name: \"GetOpInput\",\n    },\n), definitions=[]), '__signature__': <pydantic._internal._utils.LazyClassAttribute object at 0x7f72f0f02710>}",
              "return_direct": false,
              "verbose": false,
              "callbacks": null,
              "callback_manager": null,
              "tags": null,
              "metadata": null,
              "handle_tool_error": false,
              "handle_validation_error": false,
              "response_format": "content",
              "func": {},
              "coroutine": null
            }
          ]
        },
        "global_pre_tools": {},
        "global_post_tools": []
      },
      "model_name": null,
      "temperature": 0.0,
      "max_tokens": 4096,
      "tool_mode": "auto",
      "react_mode": false,
      "react_max_retries": 3
    },
    "debug_sample_file": "/tmp/translation_sample_10.jsonl",
    "origin_file_path": "/mnt/DataFlow/lz/proj/DataFlow/dataflow/example/GeneralTextPipeline/translation.jsonl",
    "pipeline_code": "\"\"\"\nAuto-generated by pipeline_assembler\n\"\"\"\nfrom dataflow.pipeline import PipelineABC\nfrom dataflow.utils.storage import FileStorage\nfrom dataflow.serving import APILLMServing_request, LocalModelLLMServing_vllm\n\nfrom dataflow.operators.core_text import EmbeddingGenerator, GeneralFilter, PandasOperator, PromptedFilter, PromptedRefiner\n\n\n\nclass RecommendPipeline(PipelineABC):\n    def __init__(self):\n        super().__init__()\n        # -------- FileStorage --------\n        self.storage = FileStorage(\n            first_entry_file_name=\"/tmp/translation_sample_10.jsonl\",\n            cache_path=\"/mnt/DataFlow/lz/proj/DataFlow-Agent/cache_dir\",\n            file_name_prefix=\"dataflow_cache_step\",\n            cache_type=\"jsonl\",\n        )\n        # -------- LLM Serving (Remote) --------\n        self.llm_serving = APILLMServing_request(\n            api_url=\"http://123.129.219.111:3000/v1/chat/completions\",\n            key_name_of_api_key=\"DF_API_KEY\",\n            model_name=\"gpt-4o\",\n            max_workers=100,\n        )\n\n        self.prompted_filter = PromptedFilter(llm_serving=self.llm_serving, system_prompt='Please evaluate the quality of this data on a scale from 1 to 5.', min_score=1, max_score=5)\n        self.general_filter = GeneralFilter(filter_rules=None)\n        self.prompted_refiner = PromptedRefiner(llm_serving=self.llm_serving, system_prompt='You are a helpful agent.')\n        self.embedding_generator = EmbeddingGenerator(embedding_serving=None)\n        self.pandas_operator = PandasOperator(process_fn=None)\n\n    def forward(self):\n        self.prompted_filter.run(\n            storage=self.storage.step(), input_key='raw_content', output_key='eval'\n        )\n        self.general_filter.run(\n            storage=self.storage.step()\n        )\n        self.prompted_refiner.run(\n            storage=self.storage.step(), input_key='raw_content'\n        )\n        self.embedding_generator.run(\n            storage=self.storage.step(), input_key='text', output_key='embeddings'\n        )\n        self.pandas_operator.run(\n            storage=self.storage.step()\n        )\n\nif __name__ == \"__main__\":\n    pipeline = RecommendPipeline()\n    pipeline.compile()\n    pipeline.forward()\n",
    "structure_code": {
      "nodes": [
        {
          "id": "node1",
          "name": "PromptedFilter",
          "type": "filter",
          "config": {
            "init": {
              "llm_serving": "self.llm_serving",
              "system_prompt": "Please evaluate the quality of this data on a scale from 1 to 5.",
              "min_score": 1,
              "max_score": 5
            },
            "run": {
              "storage": "self.storage.step()",
              "input_key": "raw_content",
              "output_key": "eval"
            }
          }
        },
        {
          "id": "node2",
          "name": "GeneralFilter",
          "type": "filter",
          "config": {
            "init": {
              "filter_rules": null
            },
            "run": {
              "storage": "self.storage.step()"
            }
          }
        },
        {
          "id": "node3",
          "name": "PromptedRefiner",
          "type": "refine",
          "config": {
            "init": {
              "llm_serving": "self.llm_serving",
              "system_prompt": "You are a helpful agent."
            },
            "run": {
              "storage": "self.storage.step()",
              "input_key": "raw_content"
            }
          }
        },
        {
          "id": "node4",
          "name": "EmbeddingGenerator",
          "type": "generate",
          "config": {
            "init": {
              "embedding_serving": null
            },
            "run": {
              "storage": "self.storage.step()",
              "input_key": "text",
              "output_key": "embeddings"
            }
          }
        },
        {
          "id": "node5",
          "name": "PandasOperator",
          "type": "refine",
          "config": {
            "init": {
              "process_fn": null
            },
            "run": {
              "storage": "self.storage.step()"
            }
          }
        }
      ],
      "edges": []
    }
  },
  "category": {
    "category": "core_text"
  },
  "recommendation": [
    "PromptedFilter",
    "GeneralFilter",
    "PromptedRefiner",
    "EmbeddingGenerator",
    "PandasOperator"
  ],
  "matched_ops": [],
  "debug_mode": true,
  "pipeline_structure_code": {
    "nodes": [
      {
        "id": "node1",
        "name": "PromptedFilter",
        "type": "filter",
        "config": {
          "init": {
            "llm_serving": "self.llm_serving",
            "system_prompt": "Please evaluate the quality of this data on a scale from 1 to 5.",
            "min_score": 1,
            "max_score": 5
          },
          "run": {
            "storage": "self.storage.step()",
            "input_key": "raw_content",
            "output_key": "eval"
          }
        }
      },
      {
        "id": "node2",
        "name": "GeneralFilter",
        "type": "filter",
        "config": {
          "init": {
            "filter_rules": null
          },
          "run": {
            "storage": "self.storage.step()"
          }
        }
      },
      {
        "id": "node3",
        "name": "PromptedRefiner",
        "type": "refine",
        "config": {
          "init": {
            "llm_serving": "self.llm_serving",
            "system_prompt": "You are a helpful agent."
          },
          "run": {
            "storage": "self.storage.step()",
            "input_key": "raw_content"
          }
        }
      },
      {
        "id": "node4",
        "name": "EmbeddingGenerator",
        "type": "generate",
        "config": {
          "init": {
            "embedding_serving": null
          },
          "run": {
            "storage": "self.storage.step()",
            "input_key": "text",
            "output_key": "embeddings"
          }
        }
      },
      {
        "id": "node5",
        "name": "PandasOperator",
        "type": "refine",
        "config": {
          "init": {
            "process_fn": null
          },
          "run": {
            "storage": "self.storage.step()"
          }
        }
      }
    ],
    "edges": []
  },
  "execution_result": {
    "success": null,
    "skipped": true,
    "file_path": "/mnt/DataFlow/lz/proj/DataFlow-Agent/dataflow_agent/tmps/dXNlcm5hbWU9bGl1emhvdQ==/my_pipeline.py"
  },
  "code_debug_result": {}
}