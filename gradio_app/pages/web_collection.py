import os
import asyncio
import logging
from typing import Optional
import gradio as gr

from dataflow_agent.states.web_collection_state import WebCollectionState, WebCollectionRequest
from dataflow_agent.workflow.wf_web_collection import create_web_collection_graph


def create_web_collection():
    """å­é¡µé¢ï¼šç½‘é¡µæ•°æ®é‡‡é›†ä¸è½¬æ¢ï¼ˆåŸºäº Web Collection å·¥ä½œæµï¼‰"""
    with gr.Blocks() as page:
        gr.Markdown("# ğŸŒ ç½‘é¡µæ•°æ®é‡‡é›†ä¸è½¬æ¢")

        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
            with gr.Column():
                gr.Markdown("### é‡‡é›†é…ç½®")
                target = gr.Textbox(
                    label="ç›®æ ‡æè¿°",
                    placeholder="ä¾‹å¦‚ï¼šæ”¶é›† Python ä»£ç ç¤ºä¾‹çš„æ•°æ®é›†",
                    lines=3
                )
                category = gr.Dropdown(
                    label="æ•°æ®ç±»åˆ«",
                    choices=["PT", "SFT"],
                    value="SFT"
                )
                output_format = gr.Dropdown(
                    label="è¾“å‡ºæ ¼å¼",
                    choices=["alpaca", "sharegpt"],
                    value="alpaca",
                    info="ç›®æ ‡è¾“å‡ºæ•°æ®æ ¼å¼"
                )
                max_download_subtasks = gr.Number(
                    label="ä¸‹è½½å­ä»»åŠ¡ä¸Šé™",
                    value=5,
                    precision=0,
                    minimum=1,
                    info="æ¯ä¸ªå­ä»»åŠ¡æœ€å¤šä¸‹è½½å­ä»»åŠ¡æ•°"
                )
                download_dir = gr.Textbox(
                    label="ä¸‹è½½ç›®å½•",
                    value="./web_collection_output",
                )
                language = gr.Dropdown(
                    label="æç¤ºè¯è¯­è¨€",
                    choices=["zh", "en"],
                    value="zh"
                )

                gr.Markdown("### LLM é…ç½®")
                chat_api_url = gr.Textbox(
                    label="DF_API_URL",
                    value=os.getenv("DF_API_URL", "")
                )
                api_key = gr.Textbox(
                    label="DF_API_KEY",
                    value=os.getenv("DF_API_KEY", ""),
                    type="password"
                )
                model = gr.Textbox(
                    label="CHAT_MODEL",
                    value=os.getenv("CHAT_MODEL", "gpt-4o")
                )

                gr.Markdown("### å…¶ä»–ç¯å¢ƒé…ç½®")
                hf_endpoint = gr.Textbox(
                    label="HF_ENDPOINT",
                    value=os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
                )
                kaggle_username = gr.Textbox(
                    label="KAGGLE_USERNAME",
                    value=os.getenv("KAGGLE_USERNAME", "")
                )
                kaggle_key = gr.Textbox(
                    label="KAGGLE_KEY",
                    value=os.getenv("KAGGLE_KEY", ""),
                    type="password"
                )
                tavily_api_key = gr.Textbox(
                    label="TAVILY_API_KEY",
                    value=os.getenv("TAVILY_API_KEY", ""),
                    type="password"
                )

                gr.Markdown("### RAG é…ç½®")
                rag_embed_model = gr.Textbox(
                    label="RAG_EMB_MODEL",
                    value=os.getenv("RAG_EMB_MODEL", "text-embedding-3-large")
                )
                rag_api_url = gr.Textbox(
                    label="RAG_API_URL",
                    value=os.getenv("RAG_API_URL", "")
                )
                rag_api_key = gr.Textbox(
                    label="RAG_API_KEY",
                    value=os.getenv("RAG_API_KEY", ""),
                    type="password"
                )

                # é«˜çº§é…ç½®åŒºåŸŸï¼ˆå¯æŠ˜å ï¼‰
                with gr.Accordion("âš™ï¸ é«˜çº§é…ç½®", open=False):
                    gr.Markdown("### æœç´¢ä¸çˆ¬å–é…ç½®")
                    search_engine = gr.Dropdown(
                        label="æœç´¢å¼•æ“",
                        choices=["tavily", "google", "bing", "duckduckgo"],
                        value="tavily",
                        info="é€‰æ‹©ç”¨äºæœç´¢çš„å¼•æ“"
                    )
                    max_urls = gr.Slider(
                        label="æœ€å¤§ URL æ•°é‡",
                        minimum=1,
                        maximum=50,
                        step=1,
                        value=10,
                        info="å•æ¬¡æœç´¢æœ€å¤§å¤„ç†URLæ•°é‡"
                    )
                    max_depth = gr.Slider(
                        label="æœ€å¤§çˆ¬å–æ·±åº¦",
                        minimum=1,
                        maximum=10,
                        step=1,
                        value=2,
                        info="çˆ¬å–æœ€å¤§æ·±åº¦"
                    )
                    enable_rag = gr.Checkbox(
                        label="å¯ç”¨ RAG å¢å¼º",
                        value=True,
                        info="æ˜¯å¦å¯ç”¨ RAG å¢å¼º"
                    )
                    concurrent_pages = gr.Slider(
                        label="WebCrawler å¹¶å‘çˆ¬å–æ•°",
                        minimum=1,
                        maximum=20,
                        step=1,
                        value=3,
                        info="WebCrawler å¹¶è¡Œå¤„ç†çš„é¡µé¢æ•°é‡"
                    )

                    gr.Markdown("### WebCrawler é…ç½®")
                    enable_webcrawler = gr.Checkbox(
                        label="å¯ç”¨ WebCrawler",
                        value=True,
                        info="æ˜¯å¦å¯ç”¨ WebCrawler å¹¶è¡Œçˆ¬å–ï¼ˆé»˜è®¤å¯ç”¨ï¼Œagent ä¼šæ ¹æ®ä»»åŠ¡æè¿°è‡ªåŠ¨è¯„ä¼°ï¼‰"
                    )
                    debug = gr.Checkbox(
                        label="è°ƒè¯•æ¨¡å¼",
                        value=False,
                        info="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—"
                    )
                    disable_cache = gr.Checkbox(
                        label="ç¦ç”¨ç¼“å­˜",
                        value=True,
                        info="å¦‚æœå¯ç”¨ï¼Œå°†å®Œå…¨ç¦ç”¨ HuggingFace å’Œ Kaggle çš„ç¼“å­˜ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•å¹¶åœ¨ä¸‹è½½åè‡ªåŠ¨æ¸…ç†"
                    )
                    temp_base_dir = gr.Textbox(
                        label="ä¸´æ—¶ç›®å½•ï¼ˆå¯é€‰ï¼‰",
                        value="",
                        placeholder="ç•™ç©ºåˆ™ä½¿ç”¨é»˜è®¤ä¸´æ—¶ç›®å½•",
                        info="è‡ªå®šä¹‰ä¸´æ—¶ç›®å½•è·¯å¾„ï¼Œç”¨äºç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶"
                    )

                submit_btn = gr.Button("å¼€å§‹ç½‘é¡µé‡‡é›†ä¸è½¬æ¢", variant="primary")

            # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
            with gr.Column():
                with gr.Tab("æ‰§è¡Œæ—¥å¿—"):
                    output_log = gr.Textbox(label="æ—¥å¿—", lines=18)
                with gr.Tab("ç»“æœæ‘˜è¦"):
                    output_json = gr.JSON(label="æ‰§è¡Œç»“æœ")

        async def run_pipeline(
            target_text: str,
            category_val: str,
            output_format_val: str,
            max_download_subtasks_val: float | None,
            download_dir_val: str,
            language_val: str,
            chat_api_url_val: str,
            api_key_val: str,
            model_val: str,
            hf_endpoint_val: str,
            kaggle_username_val: str,
            kaggle_key_val: str,
            tavily_api_key_val: str,
            rag_embed_model_val: str,
            rag_api_url_val: str,
            rag_api_key_val: str,
            # é«˜çº§é…ç½®å‚æ•°
            search_engine_val: str,
            max_urls_val: int,
            max_depth_val: int,
            enable_rag_val: bool,
            concurrent_pages_val: int,
            enable_webcrawler_val: bool,
            debug_val: bool,
            disable_cache_val: bool,
            temp_base_dir_val: str,
        ):
            # æ³¨å…¥/è¦†ç›–è¿è¡Œæ‰€éœ€çš„ç¯å¢ƒå˜é‡
            os.environ["DF_API_URL"] = chat_api_url_val or ""
            os.environ["DF_API_KEY"] = api_key_val or ""
            os.environ["CHAT_MODEL"] = model_val or ""
            os.environ["HF_ENDPOINT"] = hf_endpoint_val or ""
            os.environ["KAGGLE_USERNAME"] = kaggle_username_val or ""
            os.environ["KAGGLE_KEY"] = kaggle_key_val or ""
            os.environ["RAG_EMB_MODEL"] = rag_embed_model_val or ""
            os.environ["RAG_API_URL"] = rag_api_url_val or ""
            os.environ["RAG_API_KEY"] = rag_api_key_val or ""
            if tavily_api_key_val:
                os.environ["TAVILY_API_KEY"] = tavily_api_key_val
            else:
                os.environ.pop("TAVILY_API_KEY", None)

            # è®¾ç½®é«˜çº§é…ç½®ç›¸å…³ç¯å¢ƒå˜é‡
            if disable_cache_val:
                os.environ["DF_DISABLE_CACHE"] = "true"
            else:
                os.environ.pop("DF_DISABLE_CACHE", None)

            if temp_base_dir_val:
                os.environ["DF_TEMP_DIR"] = temp_base_dir_val
            else:
                os.environ.pop("DF_TEMP_DIR", None)

            # è§„èŒƒåŒ–ä¸‹è½½å­ä»»åŠ¡ä¸Šé™
            max_download_subtasks_int: Optional[int] = None
            if max_download_subtasks_val is not None:
                try:
                    numeric = int(max_download_subtasks_val)
                    if numeric > 0:
                        max_download_subtasks_int = numeric
                except (TypeError, ValueError):
                    pass

            # ç»„è£… WebCollectionRequest
            request = WebCollectionRequest(
                target=target_text,
                category=category_val,
                output_format=output_format_val,
                download_dir=download_dir_val,
                language=language_val,
                chat_api_url=chat_api_url_val,
                api_key=api_key_val,
                model=model_val,
                # æœç´¢é…ç½®
                search_engine=search_engine_val,
                max_urls=int(max_urls_val),
                max_depth=int(max_depth_val),
                max_download_subtasks=max_download_subtasks_int,
                # RAG é…ç½®
                enable_rag=enable_rag_val,
                rag_embed_model=rag_embed_model_val or "",
                rag_api_base_url=rag_api_url_val or None,
                rag_api_key=rag_api_key_val or None,
                # å¤–éƒ¨ API Keys
                tavily_api_key=tavily_api_key_val or None,
                kaggle_username=kaggle_username_val or None,
                kaggle_key=kaggle_key_val or None,
                # WebCrawler é…ç½®
                enable_webcrawler=enable_webcrawler_val,
                webcrawler_concurrent_pages=int(concurrent_pages_val),
                # å¤„ç†é…ç½®
                debug=debug_val,
            )

            # æ„å»ºåˆå§‹çŠ¶æ€
            state = WebCollectionState(request=request)

            # ä½¿ç”¨æ–°ç‰ˆå·¥ä½œæµå›¾
            builder = create_web_collection_graph()
            graph = builder.build()

            header_lines = [
                "=" * 60,
                "å¼€å§‹æ‰§è¡Œ Web Collection å·¥ä½œæµ",
                "=" * 60,
                f"ç›®æ ‡: {request.target}",
                f"ç±»åˆ«: {request.category}",
                f"è¾“å‡ºæ ¼å¼: {request.output_format}",
                f"ä¸‹è½½ç›®å½•: {request.download_dir}",
                "\nã€æœç´¢ä¸çˆ¬å–é…ç½®ã€‘",
                f"  - æœç´¢å¼•æ“: {search_engine_val}",
                f"  - æœ€å¤§ URL æ•°: {max_urls_val}",
                f"  - æœ€å¤§çˆ¬å–æ·±åº¦: {max_depth_val}",
                f"  - ä¸‹è½½å­ä»»åŠ¡ä¸Šé™: {max_download_subtasks_int if max_download_subtasks_int is not None else 'é»˜è®¤(5)'}",
                f"  - å¯ç”¨ RAG: {'æ˜¯' if enable_rag_val else 'å¦'}",
                "\nã€WebCrawler é…ç½®ã€‘",
                f"  - å¯ç”¨ WebCrawler: {'æ˜¯' if enable_webcrawler_val else 'å¦'}",
                f"  - å¹¶å‘çˆ¬å–æ•°: {concurrent_pages_val}",
                f"  - è°ƒè¯•æ¨¡å¼: {'æ˜¯' if debug_val else 'å¦'}",
                f"  - ç¦ç”¨ç¼“å­˜: {'æ˜¯' if disable_cache_val else 'å¦'}",
                "=" * 60,
            ]

            log_lines: list[str] = header_lines.copy()
            log_queue: asyncio.Queue = asyncio.Queue()

            class DataflowLogFilter(logging.Filter):
                def filter(self, record: logging.LogRecord) -> bool:  # type: ignore[override]
                    return record.name.startswith("dataflow_agent") or record.name.startswith("script")

            class GradioLogHandler(logging.Handler):
                def __init__(self, queue: asyncio.Queue[str]):
                    super().__init__(level=logging.INFO)
                    self.queue = queue
                    self.addFilter(DataflowLogFilter())
                    self.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))

                def emit(self, record: logging.LogRecord) -> None:  # type: ignore[override]
                    try:
                        message = self.format(record)
                        loop = asyncio.get_running_loop()
                        loop.call_soon_threadsafe(self.queue.put_nowait, message)
                    except RuntimeError:
                        try:
                            self.queue.put_nowait(self.format(record))
                        except asyncio.QueueFull:
                            pass
                    except Exception:
                        pass

            handler = GradioLogHandler(log_queue)
            root_logger = logging.getLogger()
            root_logger.addHandler(handler)
            original_level = root_logger.level
            if original_level == 0 or original_level > logging.INFO:
                root_logger.setLevel(logging.INFO)

            attached_loggers: set[logging.Logger] = {root_logger}

            def _attach_to_existing_loggers() -> None:
                logger_dict = logging.root.manager.loggerDict  # type: ignore[attr-defined]
                for name in list(logger_dict.keys()):
                    if isinstance(name, str) and (name.startswith("dataflow_agent") or name.startswith("script")):
                        logger_obj = logging.getLogger(name)
                        logger_obj.addHandler(handler)
                        attached_loggers.add(logger_obj)

                for name in ("dataflow_agent", "script"):
                    logger_obj = logging.getLogger(name)
                    logger_obj.addHandler(handler)
                    attached_loggers.add(logger_obj)

            _attach_to_existing_loggers()

            # åˆå§‹è¾“å‡º
            yield "\n".join(log_lines), gr.update(value=None)

            async def run_workflow():
                return await graph.ainvoke(state)

            workflow_task = asyncio.create_task(run_workflow())
            result_payload: Optional[dict] = None

            try:
                while True:
                    try:
                        message = await asyncio.wait_for(log_queue.get(), timeout=0.3)
                        log_lines.append(message)
                        yield "\n".join(log_lines), gr.update(value=result_payload)
                    except asyncio.TimeoutError:
                        if workflow_task.done():
                            break

                final_state = await workflow_task

                # æ¸…ç©ºå‰©ä½™æ—¥å¿—
                while True:
                    try:
                        pending = log_queue.get_nowait()
                        log_lines.append(pending)
                    except asyncio.QueueEmpty:
                        break

                log_lines.append("æµç¨‹æ‰§è¡Œå®Œæˆï¼")

                # ä»æœ€ç»ˆçŠ¶æ€æå–ç»“æœï¼ˆå…¼å®¹ dict å’Œ WebCollectionState å¯¹è±¡ï¼‰
                if isinstance(final_state, dict):
                    exception = final_state.get("exception", "")
                    mapping_results = final_state.get("mapping_results", {})
                    download_results = final_state.get("download_results", {})
                    webcrawler_summary = final_state.get("webcrawler_summary", "")
                    webcrawler_sft_jsonl_path = final_state.get("webcrawler_sft_jsonl_path", "")
                    webcrawler_pt_jsonl_path = final_state.get("webcrawler_pt_jsonl_path", "")
                else:
                    exception = getattr(final_state, "exception", "")
                    mapping_results = getattr(final_state, "mapping_results", {})
                    download_results = getattr(final_state, "download_results", {})
                    webcrawler_summary = getattr(final_state, "webcrawler_summary", "")
                    webcrawler_sft_jsonl_path = getattr(final_state, "webcrawler_sft_jsonl_path", "")
                    webcrawler_pt_jsonl_path = getattr(final_state, "webcrawler_pt_jsonl_path", "")

                if exception:
                    log_lines.append(f"è­¦å‘Š: æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {exception}")

                result_payload = {
                    "download_dir": request.download_dir,
                    "category": request.category,
                    "output_format": request.output_format,
                    "language": request.language,
                    "chat_model": request.model,
                    "max_download_subtasks": request.max_download_subtasks,
                    "enable_webcrawler": request.enable_webcrawler,
                }

                if download_results:
                    completed = download_results.get("completed", 0)
                    failed = download_results.get("failed", 0)
                    total = download_results.get("total", 0)
                    result_payload["download_stats"] = f"{completed}/{total} æˆåŠŸ, {failed} å¤±è´¥"

                if mapping_results:
                    output_path = mapping_results.get("output_file", "") or mapping_results.get("output_path", "")
                    total_mapped = mapping_results.get("mapped_records", 0) or mapping_results.get("total_mapped", 0)
                    result_payload["mapping_output_file"] = output_path
                    result_payload["mapping_total_records"] = total_mapped

                if webcrawler_sft_jsonl_path:
                    result_payload["webcrawler_sft_jsonl"] = webcrawler_sft_jsonl_path
                if webcrawler_pt_jsonl_path:
                    result_payload["webcrawler_pt_jsonl"] = webcrawler_pt_jsonl_path
                if webcrawler_summary:
                    result_payload["webcrawler_summary"] = webcrawler_summary

                yield "\n".join(log_lines), result_payload

            except Exception as exc:
                error_message = f"æµç¨‹æ‰§è¡Œå¤±è´¥: {exc}"
                log_lines.append(error_message)
                while True:
                    try:
                        pending = log_queue.get_nowait()
                        log_lines.append(pending)
                    except asyncio.QueueEmpty:
                        break
                result_payload = {"error": str(exc)}
                yield "\n".join(log_lines), result_payload
                raise
            finally:
                for logger_obj in attached_loggers:
                    logger_obj.removeHandler(handler)
                handler.close()
                root_logger.setLevel(original_level)

        submit_btn.click(
            run_pipeline,
            inputs=[
                target,
                category,
                output_format,
                max_download_subtasks,
                download_dir,
                language,
                chat_api_url,
                api_key,
                model,
                hf_endpoint,
                kaggle_username,
                kaggle_key,
                tavily_api_key,
                rag_embed_model,
                rag_api_url,
                rag_api_key,
                # é«˜çº§é…ç½®å‚æ•°
                search_engine,
                max_urls,
                max_depth,
                enable_rag,
                concurrent_pages,
                enable_webcrawler,
                debug,
                disable_cache,
                temp_base_dir,
            ],
            outputs=[output_log, output_json],
        )

    return page
